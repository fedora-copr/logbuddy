import logging
import os
from urllib.parse import urlparse
from urllib.request import urlretrieve

import requests
import progressbar

from llama_cpp import Llama
from logdetective.constants import CACHE_LOC, PROMPT_TEMPLATE


LOG = logging.getLogger("logdetective")


class MyProgressBar():
    """Show progress when downloading model."""
    def __init__(self):
        self.pbar = None

    def __call__(self, block_num, block_size, total_size):
        if not self.pbar:
            self.pbar = progressbar.ProgressBar(maxval=total_size)
            self.pbar.start()

        downloaded = block_num * block_size
        if downloaded < total_size:
            self.pbar.update(downloaded)
        else:
            self.pbar.finish()


def chunk_continues(text: str, index: int) -> bool:
    """Set of heuristics for determining whether or not
    does the current chunk of log text continue on next line.
    """
    conditionals = [
        lambda i, string: string[i + 1].isspace(),
        lambda i, string: string[i - 1] == "\\"
    ]

    for c in conditionals:
        y = c(index, text)
        if y:
            return True

    return False


def get_chunks(text: str):
    """Split log into chunks according to heuristic
    based on whitespace and backslash presence.
    """
    text_len = len(text)
    i = 0
    chunk = ""
    while i < text_len:
        chunk += text[i]
        if text[i] == '\n':
            if i + 1 < text_len and chunk_continues(text, i):
                i += 1
                continue
            yield chunk
            chunk = ""
        i += 1


def download_model(url: str, verbose: bool = False) -> str:
    """ Downloads a language model from a given URL and saves it to the cache directory.

    Args:
        url (str): The URL of the language model to be downloaded.

    Returns:
        str: The local file path of the downloaded language model.
    """
    path = os.path.join(
        os.path.expanduser(CACHE_LOC), url.split('/')[-1])

    LOG.info("Downloading model from %s to %s", url, path)
    if not os.path.exists(path):
        if verbose:
            path, _status = urlretrieve(url, path, MyProgressBar())
        else:
            path, _status = urlretrieve(url, path)

    return path


def initialize_model(model_pth: str, verbose: bool) -> Llama:
    """Initialize Llama class for inference.
    Args:
        model_pth (str): path to gguf model file
        verbose (bool): level of verbosity for llamacpp
    """
    model = Llama(
        model_path=model_pth,
        n_ctx=0,  # Maximum context for the model
        verbose=verbose)

    return model


def process_log(log: str, model: Llama) -> str:
    """
    Processes a given log using the provided language model and returns its summary.

    Args:
        log (str): The input log to be processed.
        model (Llama): The language model used for processing the log.

    Returns:
        str: The summary of the given log generated by the language model.
    """
    return model(PROMPT_TEMPLATE.format(log), max_tokens=0)["choices"][0]["text"]

def retrieve_log_content(log_path: str) -> str:
    """Get content of the file on the log_path path."""
    parsed_url = urlparse(log_path)
    log = ""

    if not parsed_url.scheme:
        if not os.path.exists(log_path):
            raise ValueError(f"Local log {log_path} doesn't exist!")

        with open(log_path, "rt") as f:
            log = f.read()

    else:
        log = requests.get(log_path, timeout=60).text

    return log
